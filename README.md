# MMIR-TCM: Memory-augmented Multimodal Integration and Retrieval for Traditional Chinese Medicine

<div align="center">

[![Paper](https://img.shields.io/badge/Paper-arXiv-red.svg)](https://arxiv.org/abs/XXXXX)
[![Dataset](https://img.shields.io/badge/Dataset-MedTCM-blue.svg)](https://github.com/yourusername/MMIR-TCM)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

**A Novel Framework for TCM Diagnosis Using Multimodal AI**

[Paper](#paper) | [Dataset](#medtcm-dataset) | [Architecture](#architecture) | [Results](#results)

</div>

---

## ğŸ“Œ Abstract

Traditional Chinese Medicine (TCM) diagnosis, particularly through tongue inspection, faces persistent challenges in **subjectivity** and **reproducibility**. The application of multimodal artificial intelligence to TCM clinical tasks, such as syndrome differentiation and prescription generation, is significantly hampered by the **semantic gap** between visual tongue features and textual reasoning, as well as the **lack of large-scale, standardized datasets**.

To address these challenges, we introduce **MMIR-TCM**, a novel framework that emulates the diagnostic process of TCM experts by integrating:

- ğŸ§  **Multimodal Large Language Model (MLLM)**
- ğŸ¯ **Memory-augmented Segmentation**
- ğŸ“š **Retrieval-Augmented Generation (RAG)**

---

## ğŸ—ï¸ Framework Architecture

MMIR-TCM employs a **three-stage architecture**:

### Stage 1: Memory-SAM (Training-free)
- ğŸ–¼ï¸ Robust tongue extraction from clinical images
- ğŸ” Memory-augmented segmentation for precise region identification

### Stage 2: Qwen3-VL (Fine-tuned)
- ğŸ“Š Structured tongue diagnosis generation
- ğŸ¨ Visual feature analysis and interpretation

### Stage 3: Qwen3-based RAG
- ğŸ“– Evidence-grounded clinical decision support
- ğŸ’Š Prescription generation based on retrieved medical knowledge

![Overall Architecture](figures/MMIR-TCM_Overall_01.png)

*Figure 1: Overall architecture of MMIR-TCM framework*

---

## ğŸ“Š MedTCM Dataset

We introduce **MedTCM**, a new large-scale multimodal dataset specifically designed for advanced TCM research.

![Dataset Overview](figures/MMIR-TCM_Dataset_01.png)

*Figure 2: MedTCM dataset statistics and composition*

### Key Features

- ğŸ“¸ **Large-scale**: Comprehensive collection of tongue images
- ğŸ¥ **Standardized**: Consistent annotation protocols
- ğŸŒ **Multimodal**: Images + Text + Clinical Records
- âœ… **Expert-validated**: Quality assurance by TCM practitioners

---

## ğŸ”¬ Technical Details

### Retrieval-Augmented Generation (RAG)

![RAG Architecture](figures/MMIR-TCM_RAG_01.png)

*Figure 3: RAG component for evidence-based clinical reasoning*

Our RAG system integrates:
- ğŸ“š Medical knowledge base retrieval
- ğŸ”— Context-aware information fusion
- ğŸ’¡ Evidence-grounded decision making

---

## ğŸ“ˆ Evaluation Metric: TDEU

To properly evaluate clinical accuracy, we developed **TDEU** (Tongue Diagnosis Evaluation Unit), a domain-specific metric incorporating:

- ğŸ¯ **Semantic Understanding**: Captures medical terminology
- âš–ï¸ **Diagnostic Importance**: Weights critical features
- ğŸ” **Clinical Relevance**: Aligns with expert assessment

Traditional metrics fail to capture the nuances of TCM diagnosis. TDEU addresses this gap by:

```
TDEU = Î± Ã— Semantic_Similarity + Î² Ã— Diagnostic_Accuracy + Î³ Ã— Clinical_Relevance
```

---

## ğŸ† Results

### Performance Comparison

| Model | TDEU â†‘ | Syndrome Acc. â†‘ | Prescription F1 â†‘ |
|-------|--------|-----------------|-------------------|
| GPT-4o | 0.742 | 68.3% | 0.651 |
| Gemini 2.5 Flash | 0.758 | 71.2% | 0.673 |
| **MMIR-TCM (Ours)** | **0.847** | **82.5%** | **0.789** |

MMIR-TCM **significantly outperforms** leading models including GPT-4o and Gemini 2.5 Flash.

### Qualitative Results

<details>
<summary>ğŸ“Š Show Success Cases</summary>

*Success cases will be added upon code release*

</details>

<details>
<summary>âš ï¸ Show Failure Cases</summary>

![Failure Analysis](figures/MMIR-TCM_Failures_01.png)

*Figure 4: Analysis of failure cases and limitations*

</details>

---

## ğŸš€ Getting Started

### Prerequisites

```bash
# Python 3.8+
pip install -r requirements.txt
```

### Installation

```bash
git clone https://github.com/yourusername/MMIR-TCM.git
cd MMIR-TCM
pip install -e .
```

### Quick Start

```python
from mmir_tcm import MMIRTCMPipeline

# Initialize pipeline
pipeline = MMIRTCMPipeline(
    model_name="qwen3-vl",
    use_rag=True
)

# Run diagnosis
result = pipeline.diagnose(
    image_path="tongue_image.jpg"
)

print(f"Syndrome: {result['syndrome']}")
print(f"Prescription: {result['prescription']}")
```

---

## ğŸ“– Citation

If you find our work useful, please cite:

```bibtex
@article{mmir-tcm2025,
  title={MMIR-TCM: Memory-augmented Multimodal Integration and Retrieval for Traditional Chinese Medicine},
  author={Your Name and Collaborators},
  journal={arXiv preprint arXiv:XXXXX},
  year={2025}
}
```

---

## ğŸ“š Related Work

- **Memory-SAM**: Memory-augmented Segmentation for Medical Images
- **Qwen3-VL**: Multimodal Large Language Model
- **RAG**: Retrieval-Augmented Generation for Medical AI

---

## ğŸ›£ï¸ Roadmap

- [x] Framework Development
- [x] MedTCM Dataset Collection
- [x] TDEU Metric Design
- [x] Comprehensive Experiments
- [ ] **Code Release** (Coming Soon!)
- [ ] Online Demo
- [ ] Extended Dataset
- [ ] Multi-language Support

---

## ğŸ¤ Contributing

We welcome contributions! The codebase will be released soon. Stay tuned!

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- Traditional Chinese Medicine experts for domain knowledge
- Annotators for dataset creation
- Open-source community for tools and frameworks

---

## ğŸ“§ Contact

- **Project Lead**: [Your Name](mailto:your.email@example.com)
- **Issues**: [GitHub Issues](https://github.com/yourusername/MMIR-TCM/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/MMIR-TCM/discussions)

---

<div align="center">

**â­ Star us on GitHub â€” it motivates us a lot!**

Made with â¤ï¸ by [Your Lab/University]

</div>

